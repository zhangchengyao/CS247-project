{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title             calling twain driver 64 bite application\n",
       "Body     advices 3rd party vendors provide 64 bite twai...\n",
       "Tags                                         c# .net twain\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_rake.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df = df[['Title', 'Body', 'Tags']]\n",
    "\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df['Title'].tolist()\n",
    "body = df['Body'].tolist()\n",
    "tag = df['Tags'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [str(t) + str(b) for t, b in zip(title, body)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "def normalise(word, lemmatizer):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    # word = stemmer.stem_word(word) #if we consider stemmer then results comes with stemmed word, but in this case word will not match with comment\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word, stopwords):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def get_terms(tree, stopwords, lemmatizer):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w, lemmatizer) for w,t in leaf if acceptable_word(w, stopwords) ]\n",
    "        yield term\n",
    "        \n",
    "def extract_noun_phrase(text):\n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    grammar = r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    toks = nltk.regexp_tokenize(text, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    tree = chunker.parse(postoks)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    terms = get_terms(tree, stop_words, lemmatizer)\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "def vectorize_keyphrase_candidates(keyphrase_candidates):\n",
    "    n = len(keyphrase_candidates) # the number of candidates\n",
    "    vocabulary = set()\n",
    "    for cand in keyphrase_candidates:\n",
    "        for word in cand:\n",
    "            vocabulary.add(word)\n",
    "    vocabulary = list(vocabulary)\n",
    "    \n",
    "    X = np.zeros((n, m))\n",
    "    for i, k in enumerate(keyphrase_candidates):\n",
    "        for w in keyphrase_candidates[k]:\n",
    "            X[i][vocabulary.index(w)] += 1\n",
    "            \n",
    "    keyphrase_candidates = [' '.join(cand) for cand in keyphrase_candidates]\n",
    "    return X, keyphrase_candidates\n",
    "\n",
    "def topic_clustering(keyphrase_candidates, X, threshold, method):\n",
    "    Y = pdist(X, 'jaccard')\n",
    "    Z = linkage(Y, method=method)\n",
    "    clusters = fcluster(Z, t=threshold, criterion='distance')\n",
    "    \n",
    "    topics = []\n",
    "    cand_to_topic = dict()\n",
    "    \n",
    "    for cluster_id in range(1, max(clusters) + 1):\n",
    "        topics.append([keyphrase_candidates[j] for j in range(len(clusters)) if clusters[j] == cluster_id])\n",
    "        \n",
    "    for i, cluster_id in enumerate(clusters):\n",
    "        cand_to_topic[keyphrase_candidates[i]] = cluster_id\n",
    "        \n",
    "    return topics, cand_to_topic\n",
    "\n",
    "def build_graph(keyphrase_candidates, topics, cand_to_topic):\n",
    "    G = nx.graph()\n",
    "    \n",
    "    G.add_nodes_from(keyphrase_candidates)\n",
    "    \n",
    "    for node_i, node_j in combinations(keyphrase_candidates, 2):\n",
    "        # discard intra-topic edges\n",
    "        if cand_to_topic[node_i] == cand_to_topic[node_j]:\n",
    "            continue\n",
    "            \n",
    "        weights = []\n",
    "        for p_i in self.candidates[node_i].offsets:\n",
    "            for p_j in self.candidates[node_j].offsets:\n",
    "\n",
    "                # compute gap\n",
    "                gap = abs(p_i - p_j)\n",
    "\n",
    "                # alter gap according to candidate length\n",
    "                if p_i < p_j:\n",
    "                    gap -= len(self.candidates[node_i].lexical_form) - 1\n",
    "                if p_j < p_i:\n",
    "                    gap -= len(self.candidates[node_j].lexical_form) - 1\n",
    "\n",
    "                weights.append(1.0 / gap)\n",
    "\n",
    "        # add weighted edges \n",
    "        if weights:\n",
    "            # node_i -> node_j\n",
    "            G.add_edge(node_i, node_j, weight=sum(weights))\n",
    "            # node_j -> node_i\n",
    "            G.add_edge(node_j, node_i, weight=sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twain']\n",
      "['bite', 'applicationadvices']\n",
      "['party', 'vendor']\n",
      "['bite', 'twain', 'implementation']\n",
      "['twain', 'api', 'net', 'application', 'compile', 'x64', 'target', 'platform']\n"
     ]
    }
   ],
   "source": [
    "terms = extract_noun_phrase(questions[0])\n",
    "for term in terms:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calling twain driver 64 bite applicationadvices 3rd party vendors provide 64 bite twain implementations need call twain api net c application compile x64 target platform'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/boudinfl/ pke\n",
    "\n",
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_keyphrases(questions):\n",
    "    results = []\n",
    "        \n",
    "    for question in questions:\n",
    "        try:\n",
    "            extractor = pke.unsupervised.MultipartiteRank()\n",
    "            extractor.load_document(input=question)\n",
    "            pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    #         pos = {'NOUN'}\n",
    "            stoplist = list(string.punctuation)\n",
    "            stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "            stoplist += stopwords.words('english')\n",
    "            extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "            extractor.candidate_weighting(alpha=1.1,\n",
    "                                                  threshold=0.74,\n",
    "                                                  method='average')\n",
    "\n",
    "            keyphrases = extractor.get_n_best(n=3)\n",
    "\n",
    "            results.append([k for (k, score) in keyphrases])\n",
    "        except Exception as e:\n",
    "            results.append([])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 1 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n"
     ]
    }
   ],
   "source": [
    "results = extract_keyphrases(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bite applicationadvices', '3rd party vendors', 'twain api net'], ['variable', 'struct', 'define managerh file instantiate managerc'], ['val jcropapisetselect', 'click function', 'input text field problem'], ['workfirst name label', 'month php echo row', 'input name bfname'], ['descend order thisnamecompareto sgetname return', 'ways', 'sort list edge name'], ['multiple definitions function error', 'first define', 'ang3'], ['accentuate letter column webservice', 'call webservice j2me application exception', 'select columns'], ['url http abccomradiofrmypersonaspx', 'jquery change replace urlprinciple script', 'abccomradiofrintranetlayoutsrfportalwebpersonaspx accountnameradiofradministrateur'], ['cv mat', 'dimension matrix use svdtry', 'simple source code example'], ['auto', 'value return ajax call', 'textbox restrict user']]\n"
     ]
    }
   ],
   "source": [
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['c#', '.net', 'twain'], ['c', 'struct'], ['javascript', 'jquery', 'html'], ['javascript', 'php', 'html'], ['java'], ['c++', 'object', 'makefile'], ['php', 'mysql', 'web-services', 'java-me', 'wsdl'], ['jquery', 'replace'], ['c++', 'opencv', 'matrix', 'dimensionality-reduction'], ['javascript', 'ajax', 'jquery', 'autocomplete', 'autosuggest']]\n"
     ]
    }
   ],
   "source": [
    "tags = [str(t).split() for t in tag]\n",
    "print(tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.15128719491808756\n",
      "Recall:  0.15093614175860914\n",
      "0.1511114644521724\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for i in range(len(results)):\n",
    "    if len(results[i]) > 0:\n",
    "        num_same = 0\n",
    "        for phrase in results[i]:\n",
    "            num_same += len(set(phrase.split()) & set(tags[i]))\n",
    "        precisions.append(num_same / float(len(results[i])))\n",
    "        recalls.append(num_same / float(len(tags[i])))\n",
    "    \n",
    "precision = np.mean(precisions)\n",
    "recall = np.mean(recalls)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
