{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_rake.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df = df[['Title', 'Body', 'Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title             calling twain driver 64 bite application\n",
       "Body     advices 3rd party vendors provide 64 bite twai...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df['Title'].tolist()\n",
    "body = df['Body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [str(t) + str(b) for t, b in zip(title, body)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "def normalise(word, lemmatizer):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    # word = stemmer.stem_word(word) #if we consider stemmer then results comes with stemmed word, but in this case word will not match with comment\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word, stopwords):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def get_terms(tree, stopwords, lemmatizer):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w, lemmatizer) for w,t in leaf if acceptable_word(w, stopwords) ]\n",
    "        yield term\n",
    "        \n",
    "def extract_noun_phrase(text):\n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    grammar = r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    toks = nltk.regexp_tokenize(text, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    tree = chunker.parse(postoks)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    terms = get_terms(tree, stop_words, lemmatizer)\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "def vectorize_keyphrase_candidates(keyphrase_candidates):\n",
    "    n = len(keyphrase_candidates) # the number of candidates\n",
    "    vocabulary = set()\n",
    "    for cand in keyphrase_candidates:\n",
    "        for word in cand:\n",
    "            vocabulary.add(word)\n",
    "    vocabulary = list(vocabulary)\n",
    "    \n",
    "    X = np.zeros((n, m))\n",
    "    for i, k in enumerate(keyphrase_candidates):\n",
    "        for w in keyphrase_candidates[k]:\n",
    "            X[i][vocabulary.index(w)] += 1\n",
    "            \n",
    "    keyphrase_candidates = [' '.join(cand) for cand in keyphrase_candidates]\n",
    "    return X, keyphrase_candidates\n",
    "\n",
    "def topic_clustering(keyphrase_candidates, X, threshold, method):\n",
    "    Y = pdist(X, 'jaccard')\n",
    "    Z = linkage(Y, method=method)\n",
    "    clusters = fcluster(Z, t=threshold, criterion='distance')\n",
    "    \n",
    "    topics = []\n",
    "    cand_to_topic = dict()\n",
    "    \n",
    "    for cluster_id in range(1, max(clusters) + 1):\n",
    "        topics.append([keyphrase_candidates[j] for j in range(len(clusters)) if clusters[j] == cluster_id])\n",
    "        \n",
    "    for i, cluster_id in enumerate(clusters):\n",
    "        cand_to_topic[keyphrase_candidates[i]] = cluster_id\n",
    "        \n",
    "    return topics, cand_to_topic\n",
    "\n",
    "def build_graph(keyphrase_candidates, topics, cand_to_topic):\n",
    "    G = nx.graph()\n",
    "    \n",
    "    G.add_nodes_from(keyphrase_candidates)\n",
    "    \n",
    "    for node_i, node_j in combinations(keyphrase_candidates, 2):\n",
    "        # discard intra-topic edges\n",
    "        if cand_to_topic[node_i] == cand_to_topic[node_j]:\n",
    "            continue\n",
    "            \n",
    "        weights = []\n",
    "        for p_i in self.candidates[node_i].offsets:\n",
    "            for p_j in self.candidates[node_j].offsets:\n",
    "\n",
    "                # compute gap\n",
    "                gap = abs(p_i - p_j)\n",
    "\n",
    "                # alter gap according to candidate length\n",
    "                if p_i < p_j:\n",
    "                    gap -= len(self.candidates[node_i].lexical_form) - 1\n",
    "                if p_j < p_i:\n",
    "                    gap -= len(self.candidates[node_j].lexical_form) - 1\n",
    "\n",
    "                weights.append(1.0 / gap)\n",
    "\n",
    "        # add weighted edges \n",
    "        if weights:\n",
    "            # node_i -> node_j\n",
    "            G.add_edge(node_i, node_j, weight=sum(weights))\n",
    "            # node_j -> node_i\n",
    "            G.add_edge(node_j, node_i, weight=sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twain']\n",
      "['bite', 'applicationadvices']\n",
      "['party', 'vendor']\n",
      "['bite', 'twain', 'implementation']\n",
      "['twain', 'api', 'net', 'application', 'compile', 'x64', 'target', 'platform']\n"
     ]
    }
   ],
   "source": [
    "terms = extract_noun_phrase(questions[0])\n",
    "for term in terms:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calling twain driver 64 bite applicationadvices 3rd party vendors provide 64 bite twain implementations need call twain api net c application compile x64 target platform'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../stacksample/Questions.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df2 = df2[['Title', 'Body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_title = df['Title'].tolist()[:10000]\n",
    "raw_body = df['Body'].tolist()[:10000]\n",
    "\n",
    "raw_questions = [str(t) + str(b) for t, b in zip(title, body)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling twain driver 64 bite applicationadvices 3rd party vendors provide 64 bite twain implementations need call twain api net c application compile x64 target platform\n"
     ]
    }
   ],
   "source": [
    "print(raw_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_keyphrases(questions):\n",
    "    results = []\n",
    "        \n",
    "    for question in questions:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "        extractor.load_document(input=question)\n",
    "        # pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "        pos = {'NOUN'}\n",
    "        stoplist = list(string.punctuation)\n",
    "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "        extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "        extractor.candidate_weighting(alpha=1.1,\n",
    "                                              threshold=0.74,\n",
    "                                              method='average')\n",
    "\n",
    "        keyphrases = extractor.get_n_best(n=3)\n",
    "        \n",
    "        results.append([k for (k, score) in keyphrases])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n",
      "WARNING:root:Not enough candidates to choose from (3 requested, 2 given)\n"
     ]
    }
   ],
   "source": [
    "results = extract_keyphrases(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
